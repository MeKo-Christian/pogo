# Porting OAR-OCR to Go (Golang)

## Overview and Goals

The goal is to reimplement the **OAR-OCR** pipeline (Optical Character Recognition in Rust/ONNX) in Go. OAR-OCR provides a complete OCR solution: it performs **text detection** (finding text regions in images) and **text recognition** (transcribing text from those regions)[\[1\]](https://docs.rs/oar-ocr/latest/oar_ocr/#:~:text=,Document%20Rectification%3A%20Fix%20perspective%20distortion). It also has optional components like document orientation detection and image rectification for skewed pages[\[1\]](https://docs.rs/oar-ocr/latest/oar_ocr/#:~:text=,Document%20Rectification%3A%20Fix%20perspective%20distortion). Our focus is **inference-only** (using pre-trained models, no training) and delivering a solution that can run as a server service and as a CLI tool (similar to PaddleOCR's command-line usage). We'll also consider adding **PDF support** (processing PDFs by extracting images) as an extension.

**Key objectives:**

- **Functional Parity:** Reproduce the full OCR pipeline from image input to text output, matching OAR-OCR's accuracy and features (text detection & recognition, and optionally orientation correction).
- **Use Pre-trained Models:** Leverage the same PaddleOCR models that OAR-OCR uses for detection and recognition[\[2\]](https://github.com/greatv/oar-ocr#:~:text=,engine%20in%20this%20OCR%20library), to maintain high accuracy without retraining. These models are available as ONNX files (e.g. PP-OCRv4/v5 detection and recognition models[\[3\]](https://github.com/greatv/oar-ocr#:~:text=Model%20Type%20Version%20Category%20Model,precision%20requirements)[\[4\]](https://github.com/greatv/oar-ocr#:~:text=)) and corresponding character dictionaries for text decoding[\[5\]](https://github.com/greatv/oar-ocr#:~:text=Character%20Dictionaries).
- **Performance:** Achieve efficient inference (potentially via hardware acceleration). OAR-OCR integrates ONNX Runtime for fast inference[\[6\]](https://docs.rs/oar-ocr/latest/oar_ocr/#:~:text=,Runtime%20integration%20for%20fast%20inference), so our Go port should also utilize a high-performance inference engine.
- **Pure Go Implementation:** Aim for an idiomatic Go solution. It's acceptable to use C/C++ libraries via cgo initially (for expedience and correctness), but preference is to move toward a pure Go implementation long-term. For example, we might start with ONNX Runtime (C++ backend) and later transition to a native Go ML library (like GoMLX or Gorgonia) once the functionality is verified.
- **Testing & Reliability:** Develop comprehensive tests at each stage to ensure the ported pipeline is correct. Where possible, compare outputs with the original OAR-OCR or PaddleOCR (as a reference) on sample inputs to verify accuracy. We want high confidence that the Go version matches the behavior of the original library.

Below, we outline the technology stack and then a step-by-step plan covering implementation and testing.

## Technology Stack and Libraries

**Model Inference Engine:** We will use **ONNX Runtime** for running the neural network models in Go. Specifically, we can integrate the **onnxruntime-go** wrapper[\[7\]](https://github.com/yalue/onnxruntime_go#:~:text=This%20library%20seeks%20to%20provide,simple%20to%20use%20as%20possible), which provides Go bindings to the ONNX Runtime C API. This allows loading ONNX models and executing them with minimal overhead, leveraging the same optimized C/C++ runtime that OAR uses. The onnxruntime-go library supports multiple platforms and even GPU acceleration if the ONNX Runtime is built with CUDA[\[8\]](https://github.com/yalue/onnxruntime_go#:~:text=multiple%20Tensor%20data%20types%3B%20see,functions). Using ONNX Runtime ensures we can run the PaddleOCR models efficiently and with the same numerical results as the Rust version (which also relies on ONNX Runtime).

- _Rationale:_ ONNX Runtime is a production-grade engine for ONNX models, so it covers all necessary operators and provides optimizations. This saves us from reimplementing complex neural net operations. OAR-OCR itself explicitly built on ONNX Runtime for efficiency[\[6\]](https://docs.rs/oar-ocr/latest/oar_ocr/#:~:text=,Runtime%20integration%20for%20fast%20inference), so we follow that approach in Go for parity.

**Alternative Pure-Go ML (Long-term):** In the long run, to reduce external dependencies (cgo), we will evaluate using a Go-native ML library. A promising option is **GoMLX** (an accelerated ML framework for Go) which aims to be "a PyTorch/JAX/TensorFlow for Go"[\[9\]](https://github.com/gomlx/gomlx#:~:text=GoMLX%20is%20an%20easy,as%20a%20PyTorch%2FJax%2FTensorFlow%20for%20Go), built on top of OpenXLA for just-in-time compiled execution on CPU/GPU[\[10\]](https://github.com/gomlx/gomlx#:~:text=It%20runs%20almost%20everywhere%20Go,see%20Tamago). GoMLX provides a full suite of tensor operations and could potentially allow us to construct the OCR model graph in Go and load weights. Another option is **Gorgonia** with the ONNX-Go importer[\[11\]](https://github.com/oramasearch/onnx-go#:~:text=The%20implementation%20of%20the%20the,existent%20for%20the%20export)[\[12\]](https://github.com/oramasearch/onnx-go#:~:text=import%20%22github.com%2Fowulveryck%2Fonnx) - this can import ONNX models into a Go computational graph. However, ONNX-Go/Gorgonia currently do not support all ops and might require implementing missing layers[\[12\]](https://github.com/oramasearch/onnx-go#:~:text=import%20%22github.com%2Fowulveryck%2Fonnx). As a result, the initial implementation will stick with ONNX Runtime for reliability. Later, once the pipeline is working and tested, we can attempt to replace the ONNX Runtime calls with GoMLX or Gorgonia, if feasible, to achieve a pure Go solution. This refactoring will be easier when we have a reference output and tests to ensure correctness.

**Image Processing:** For image handling, we will use pure Go libraries where possible:

- The Go standard library (image package) will be used for image loading/decoding (JPEG, PNG, etc.) and basic image operations. We might use golang.org/x/image packages or the popular **disintegration/imaging** library for conveniences like resizing and rotation. These provide high-quality resampling algorithms for resizing, which is important to prepare images for the neural network.
- **OpenCV (gocv)** is an alternative for image processing (it has contour detection, etc.), but pulling in OpenCV via cgo is heavy. We prefer to implement necessary image ops in Go to align with the "pure Go" goal. Basic tasks like resizing, normalization, and even finding connected components can be done in pure Go given moderate image sizes. If we encounter performance issues in critical loops (e.g., contour detection), we can consider using gocv or writing optimized routines (possibly with SIMD via golang.org/x/exp/simd in future), but initially we aim to keep it simple.

**PDF Handling:** To support PDF input, we will integrate a library to extract images from PDF files. A good choice is **pdfcpu** - a PDF processing library written in Go[\[13\]](https://github.com/pdfcpu/pdfcpu#:~:text=pdfcpu%2Fpdfcpu%3A%20A%20PDF%20processor%20written,line%20interface%20%28CLI). Pdfcpu's extract API can pull out embedded images from each page of a PDF[\[14\]](https://pkg.go.dev/github.com/hhrutter/pdfcpu/extract#:~:text=Overview%20%C2%B6). This means if the PDF is a scanned document (each page is an image), we can extract that image and feed it to the OCR pipeline. Pdfcpu is pure Go and Apache-2.0 licensed, aligning well with our project. Alternatively, for PDFs that contain vector text (not scanned images), using OCR is unnecessary (text can be extracted directly); but our focus is on images, so assuming scanned PDFs is fine. We will use pdfcpu's ExtractImages functionality to get image bytes, convert to image.Image, and then apply OCR.

_(Note: Another approach is using_ _gen2brain/go-fitz_ _(MuPDF wrapper) to render PDF pages to images. However, go-fitz relies on MuPDF C library and is AGPL-licensed, which is problematic for commercial use_[_\[15\]_](https://www.libhunt.com/r/go-fitz#:~:text=Go,The%20primary)_. Pdfcpu, being pure Go and MIT, is preferable. The downside is pdfcpu extracts images but doesn't_ _render_ _vector content. We assume PDFs we care about are scanned or already have images.)_

**Character Dictionaries:** OAR/OCR uses character dictionary text files to map model outputs to actual characters[\[5\]](https://github.com/greatv/oar-ocr#:~:text=Character%20Dictionaries). We will carry over this approach. We'll load the appropriate dictionary file (e.g., ppocr_keys_v1.txt for English or others for different languages[\[16\]](https://github.com/greatv/oar-ocr#:~:text=match%20at%20L476%20Character%20Dictionary,OCRv5%20Eastern%20Slavic%20models)) into a Go slice of strings. This is straightforward using Go file I/O. The dictionary will be used in the recognition decoding step to translate model output indices into human-readable text.

**Project Structure:** We will mirror OAR's modular design[\[17\]](https://docs.rs/oar-ocr/latest/oar_ocr/#:~:text=%C2%A7Components) in our Go project for clarity and testability:

- A core **OCR pipeline** struct (similar to OAROCR in Rust) that holds sub-components and orchestrates end-to-end processing (from image to text results).
- Separate modules/classes for **Text Detection** and **Text Recognition** (and others like orientation if implemented) that encapsulate model loading, inference, and post-processing for each stage. For example, a Detector type with a method Detect(image) -> \[\]TextRegion and a Recognizer type with Recognize(croppedImage) -> (text, confidence).
- Utility functions for image operations (resizing, normalization, etc.) and for things like CTC decoding of the recognition output.
- A CLI command (could use Go's flag or spf13/cobra for fancy CLI) for running the OCR on inputs, and possibly a simple HTTP server for a service mode.

Using this structure, each component can be developed and tested in isolation (e.g., we can unit-test the detector and recognizer separately), and then integrated in the pipeline.

## Step-by-Step Implementation Plan

### 1. Preparation and Environment Setup

**1.1. Get Models and Resources:** Obtain the pre-trained ONNX model files for text detection and recognition, as well as the character dictionary. We can start with a known working set, for example: - **Detection model:** PaddleOCR's PP-OCRv4 or PP-OCRv5 detection model (mobile or server variant). For development, the smaller mobile model (e.g. ppocrv4_mobile_det.onnx, ~5 MB) is convenient[\[3\]](https://github.com/greatv/oar-ocr#:~:text=Model%20Type%20Version%20Category%20Model,precision%20requirements). It finds text regions with reasonable accuracy and is lightweight. - **Recognition model:** A corresponding recognition model, e.g. an English PP-OCRv4 mobile recognition model (en_ppocrv4_mobile_rec.onnx, ~7.7 MB)[\[18\]](https://github.com/greatv/oar-ocr#:~:text=%23%20Language) and its char dictionary (en_dict.txt). - (Optional) If we plan to include orientation detection, download the orientation classifier model (e.g., pplcnet_x1_0_doc_ori.onnx for document orientation[\[19\]](https://github.com/greatv/oar-ocr#:~:text=Model%20Type%20Version%20Category%20Model,6MB%20Fix%20perspective%20distortion)). This model predicts 0°, 90°, 180°, 270° rotations. - Verify model compatibility: ONNX models should be compatible with onnxruntime-go (which supports ONNX ops up to the version it's built for). Our chosen models are known to be used with ONNX Runtime (as OAR uses them), so it should be fine. Keep the model files in a known path or bundle them with the application (possibly via a separate download step or as part of release assets).

**1.2. Setup Go Module:** Initialize a new Go module for the project (e.g., github.com/meko-christian/ocrgo). Add dependencies: - github.com/yalue/onnxruntime_go (or an equivalent ONNX runtime Go binding). We'll follow the instructions from the repo to ensure the ONNX Runtime shared library is available. (This might involve downloading the ONNX Runtime binary for your platform and setting an env var or path so it can be loaded by the Go code[\[20\]](https://github.com/yalue/onnxruntime_go#:~:text=Note%20on%20onnxruntime%20Library%20Versions).) - Image libraries: possibly add github.com/disintegration/imaging for image resizing and image/jpeg etc. for decoding images. - github.com/pdfcpu/pdfcpu for PDF support (we might add this later when implementing PDF handling). - If using Cobra for CLI, add github.com/spf13/cobra. Otherwise, no extra dep needed for basic flag usage. - Confirm the environment builds correctly and that cgo is enabled (onnxruntime-go will require cgo). Write a very small test program to load an ONNX model with onnxruntime-go and run a dummy inference to verify everything is wired properly. For example, we could write a test that loads the orientation classifier model (small) and runs it on an empty image just to see we get an output without errors.

**1.3. Plan Testing Strategy Upfront:** Set up the project with a /testdata folder where we can store a few sample images and maybe expected outputs. Identify or create test images: - Simple cases like an image with a single word or known text (we can generate one by drawing text on a blank background). - A snippet of a scanned document image for an integration test. - Perhaps use images from PaddleOCR's examples or the OAR-OCR repository's examples (if any). We will use these later to validate the pipeline. Decide on how to store expected results (we might manually annotate expected text, or run the original OAR-OCR or PaddleOCR on them to get a reference JSON output).

With setup done, we proceed to implementation of each component:

### 2. Image Loading and Preprocessing

**2.1. Image Decoding:** Implement a utility to load an image from file path into a Go image.Image. We can use os.Open + image.Decode which supports JPEG/PNG/BMP by default. This will yield an image.Image in Go. We'll also provide a function to load multiple images (for batch processing) into a slice. This mirrors OAR's utils::load_image and load_images_batch[\[21\]](https://docs.rs/oar-ocr/latest/oar_ocr/#:~:text=%2F%2F%20Process%20single%20image%20let,0)[\[22\]](https://docs.rs/oar-ocr/latest/oar_ocr/#:~:text=%2F%2F%20Process%20multiple%20images%20let,result.index%2C%20result.text_regions.len). Write simple unit tests for these loaders (e.g., load a known PNG and check its dimensions, ensure no error on valid file, etc.).

**2.2. Resizing and Rescaling:** The detection model likely expects the input image to be scaled to a certain size or at least to fit within a maximum size. PaddleOCR's practice is to **resize the image so that its largest side does not exceed a certain value** (e.g., 960 or 1024 pixels for server models, smaller for mobile) and that the dimensions are multiples of 32 (since the detection network (DB) uses downsample factors) for optimal results. We will implement a resizing function to handle this: - Calculate scale factor so that max(width, height) = MaxSize (configurable, e.g., 960 for high-precision or 640 for faster operation). - Scale both dimensions accordingly (preserving aspect ratio). If needed, round them to a multiple of 32 (you can round _up_ to nearest 32 and then pad the image borders to that size). - Use imaging.Resize or imaging.Fit (with Lanczos filter for quality) to resize the image. The output is an RGBA image by default from that library. - If padding is required (to reach multiples of 32), we can create a new blank image of target size and draw the resized image onto it, filling the margin with a background color. Likely, we use black (pixel value 0) as background, since the models are usually trained on black background for padded areas. (We will confirm by checking PaddleOCR - it typically uses padding with zeros for the network input.)

Test this resizing on a sample image to ensure the output size is as expected and the aspect ratio is maintained.

**2.3. Normalization and Tensor Conversion:** The models expect a tensor input of shape \[1, C, H, W\] (batch size 1, channels, height, width). Usually, PaddleOCR models use 3-channel RGB images. We must convert the Go image (which might be RGBA) to an array of float32 of appropriate shape: - Convert image to **RGB** format. We might use imaging.Clone and then ensure we have R, G, B channels. Alternatively, iterate over pixels with img.At(x,y) and extract R,G,B. - Normalize pixel values: PaddleOCR typically scales pixel values to \[0,1\] by dividing by 255. It may not subtract mean or do per-channel normalization for these OCR models (they often just use simple scaling). We will assume **divide by 255** normalization unless the model documentation specifies mean/std (PaddleOCR detection/recognition by default uses no mean subtraction, just scaling). - Layout: ONNX Runtime (with default tensor) uses row-major order. The model likely expects NCHW (channels-first) format because Paddle models usually do. So we should create the float32 slice in C (row-major) order corresponding to NCHW. That means the slice layout should be: all channel 0 (R) pixels, then channel 1 (G), then B, each channel scanning rows. We will generate this in Go by nested loops or using a library function if available. (One trick: we could leverage onnxruntime_go.NewTensor which might accept a Go \[\]\[\]\[\]float32, but it likely wants a flat slice. We'll compose the flat slice manually.) - Create an ONNX tensor from this data: using onnxruntime.NewTensor&lt;Float&gt; specifying shape \[1, 3, H, W\]. The onnxruntime-go library has generic tensor creation for basic types[\[23\]](https://github.com/yalue/onnxruntime_go#:~:text=Additionally%2C%20this%20library%20uses%20Go%27s,functions).

We will write a **unit test** for this conversion function: e.g., feed a tiny 2x2 image with known pixel values and verify that the resulting tensor data matches expectations (correct ordering and normalization). This will catch any channel ordering confusion.

### 3. Text Detection Component

**3.1. Model Session Initialization:** Using onnxruntime-go, load the detection ONNX model into a session. The library typically provides a NewSession(provider, modelPath) or similar. We will wrap this in a Detector struct (e.g., type Detector struct { session \*onnxruntime.Session; inputName string; outputName string; /\* maybe other config like threshold \*/ }). We can obtain the model input/output names via the ONNX API if needed, or hardcode if known (for PaddleOCR models, often the input tensor is named "x" or "images" and output like "sigmoid_0.tmp_0" or something - but onnxruntime can fetch by index 0 as well).

We'll also allow configuring some parameters: e.g., detection threshold values. PaddleOCR's DB post-processing uses two thresholds: det_db_thresh (binary threshold for the probability map, ~0.3) and det_db_box_thresh (confidence to filter final boxes, ~0.5). We can set default values (from PaddleOCR) and allow override if needed. The OAR library likely uses defaults internally and exposes some via builder (they show text_rec_score_thresh for recognition confidence[\[24\]](https://docs.rs/oar-ocr/latest/oar_ocr/#:~:text=,build), but not explicitly for det; we'll assume 0.3/0.5 are reasonable defaults).

**3.2. Running Detection Inference:** Implement a method Detector.Detect(img image.Image) -> \[\]DetectionResult. This will: - Preprocess the input image: apply the resizing & normalization from step 2.2/2.3. - Run the ONNX runtime session: feed the float tensor to the model. The model's output for PaddleOCR detection is a single 2D map (or 3D with channel=1) of size roughly equal to the input resized image (or downscaled by a factor if the model outputs at lower resolution). In DB, typically the model outputs a heatmap where each pixel's value ~ probability of text. For example, output shape could be (1,1,H',W') with values in \[0,1\] after sigmoid. - Retrieve the output tensor from the session. With onnxruntime-go, we'll get a slice of floats. Reshape it to a 2D array for ease of processing (we know H' and W' from the model or by reading tensor shape). - **Post-processing (Contour Extraction):** This is the critical part to get bounding boxes from the heatmap. We will replicate the **Differentiable Binarization (DB) algorithm** used by PaddleOCR: 1. **Thresholding:** Apply a binary threshold to the heatmap. Use det*db_thresh (say 0.3) - any pixel with probability >= 0.3 is considered part of text region. This yields a binary mask. 2. **Find connected components (text regions):** Traverse the binary mask to find connected pixels (4-connectivity is fine). Each connected region represents one text instance (line or word block). We can implement this via a DFS/BFS flood fill or union-find. Given typical image sizes (e.g., 640x480 after resize), this is manageable in Go. We will gather the coordinates of each region's pixels. 3. **Contour & Bounding Polygon:** For each connected region, we need to derive a polygon that tightly encloses it. A simple approach is to compute the axis-aligned bounding box (min and max x,y). However, for rotated text or irregular shapes, an axis-aligned box may include a lot of background. A better approach is to find the \_contour* (the set of boundary pixels). We can implement a border-following algorithm (e.g., Moore-Neighbor tracing or Suzuki's algorithm as in OpenCV's findContours). For an initial version, we might skip directly to the convex hull of the region's pixels or the min-area rotated rectangle. But to keep it simpler: - We can approximate the region by a polygon with a certain number of points. In PaddleOCR, after finding contours, they often use an algorithm to **"unclip"** the polygon (expanding it slightly to account for text region margins)[\[19\]](https://github.com/greatv/oar-ocr#:~:text=Model%20Type%20Version%20Category%20Model,6MB%20Fix%20perspective%20distortion). We can initially not do complex unclipping and just use the contour as found. - One straightforward way: use the region's pixel list to compute a convex hull (Graham scan or similar). The convex hull will outline the region. This might be sufficient for many cases (text is usually convex-ish). Alternatively, implement a rotating calipers to get a min-area rectangle. - For now, let's plan on at least getting the **bounding box** (minX, minY, maxX, maxY in the resized image coordinate) as a coarse result. We'll improve it to a polygon or rotated box if time permits. (Mark this as an improvement in the plan.) 4. **Calculate region confidence (optional):** We can compute a confidence score for each detected region - for example, the average heatmap value within that region or the maximum value. PaddleOCR uses det_db_box_thresh (like 0.5) to filter out regions whose average score is below this. We will do similarly: for each region, calculate mean probability from the heatmap; if it's below 0.5, discard the region (it's likely noise). This helps eliminate false positives. 5. **Map coordinates back to original image size:** Because we resized the image for detection, we need to scale the detected box coordinates back to the original image's coordinate space. We know the scale used in preprocessing, so just multiply accordingly (and also adjust if padding was added). This way, the final output locations correspond to the original image (useful for drawing boxes or for users to interpret coordinates relative to input image).

- Package each detected region into a struct (say TextRegion{Polygon \[\]Point, BoundingBox Rectangle, confidence float} etc.). If we only have axis-aligned box for now, that's fine; we can provide polygon as four corners of the box. Ultimately, we want to feed these regions into the recognizer.

Write **unit tests** for the core of post-processing: - We can synthetic-test the contour finder by creating a small binary matrix and ensuring it finds the right number of components and shapes. For example, make a 10x10 grid with a filled rectangle of 1s and see if our algorithm finds one region and correct bounds. - We might also test the scaling of coordinates by giving a known scale and ensuring the mapping is correct.

Additionally, after integration, we will do an **integration test**: run the detector on a real image (from testdata) and verify it finds a reasonable number of text boxes (we can't hardcode exact values easily, but we can at least assert non-zero detections for an image known to contain text, or compare with expected count).

### 4. Text Recognition Component

**4.1. Model Session Initialization:** Set up the recognizer similar to detector. A Recognizer struct holding an ONNX Runtime session for the recognition model and the character dictionary mapping. This model will take a cropped text image as input and output a sequence (probability distribution per character position). For PaddleOCR CRNN models, the output is usually of shape \[1, seq_len, num_classes\] or \[1, num_classes, seq_len\] depending on how it's arranged. We'll find out by inspecting the model or trying a run. Typically, num_classes equals the size of the character set plus 1 (for the CTC "blank" symbol). For example, if the dictionary has 95 characters, num_classes might be 96 with blank.

**4.2. Preprocessing Cropped Regions:** The text regions from detection need to be fed to the recognizer. We must preprocess each crop: - **Crop extraction:** Use Go's image draw functions to extract the sub-image corresponding to the detected region. If the region is a polygon (rotated), we might need to **rotate** that crop to horizontal orientation. PaddleOCR assumes the text is roughly horizontal (they have a separate angle classifier for rotated text lines). We plan to implement an optional text line orientation classifier later. For now, if we assume mostly horizontal text, we can take the bounding box of the polygon and crop that. If the text is rotated 90 degrees, our recognition might fail unless we detect and rotate it; we will handle that in the orientation step. - **Resize for recognition:** Recognition models usually require a fixed height (e.g., 32 pixels) and flexible width. PaddleOCR's recognition model expects a 32x? or 48x? image. We will resize the crop's height to the model's input height (check the ONNX model input shape; often 3x32xWidth or 3x48xWidth). Keep aspect ratio when scaling the width, up to a max width (maybe 320 or 256 pixels). If the text image is shorter, PaddleOCR sometimes pads it to a fixed width. The ONNX might have a dynamic axis for width, which would allow variable widths per inference. If not, we may have to pad or truncate. The safer route: find the model's expected input shape from its metadata. If it's dynamic, good. If it's fixed (e.g., 3x32x320), then we must scale and pad the image to exactly that. - Use similar normalization as for detector: scale to \[0,1\] floats. (PaddleOCR recognition also typically just divides by 255.)

We will implement a helper for this and test it on an artificial small image (like a single character image) to ensure it outputs correct tensor shape.

**4.3. Running Recognition Model:** For each text crop (after preprocessing to the right tensor shape), run the ONNX session: - Feed the image tensor, get the output. The output is a sequence of character probabilities. For example, shape could be \[1, seq_len, 96\] if 96 classes (including blank). We'll get this as a float array. - We need to **decode** this sequence to actual text. If the model uses **CTC (Connectionist Temporal Classification)** (likely, given PaddleOCR mobile models use CTC), the decoding algorithm is: - For each time step in the sequence, find the index with highest probability (argmax). This yields a raw sequence of class indices. - Collapse repeating indices and remove any occurrences of the "blank" index (which usually is index 0 or the last index depending on convention). The dictionary file typically does **not** include an entry for blank; blank is an extra class. We need to confirm how the dictionary aligns. For instance, PaddleOCR's ppocr_keys_v1.txt has 36 alphanumeric + punctuation etc. The model might output blank as index 0 and characters starting from index 1, or vice versa. We can figure out by checking if the output mostly yields 0 when no text. We will adjust accordingly (this can be done by trial with a simple input). - After CTC collapse, we get the final character sequence indices. Map each index to the actual character via our dictionary slice. Assemble the string. - Calculate a confidence score for the text: We can use the probabilities from the model to estimate confidence. A common approach is to take the probability of each chosen character at the time step and average them. OAR-OCR reports a confidence per region[\[25\]](https://github.com/greatv/oar-ocr#:~:text=for%20text_region%20in%20%26result.text_regions%20,text%2C%20confidence%29%3B%20%7D)[\[26\]](https://github.com/greatv/oar-ocr#:~:text=for%20text_region%20in%20%26result.text_regions%20,), which likely corresponds to recognition confidence. We'll implement: for each output char, look up its probability (the softmax value) in that time step, then take the **mean probability** across all chars in the sequence as the confidence. (If needed, or if using CTC, ignoring blank steps in this average.) - If the model uses an attention decoder instead of CTC (some newer PaddleOCR versions have an attention-based recognition for certain models), the output might already be a character sequence (with an end-of-sequence token). But since we're using PP-OCRv4 mobile, it's CTC. We will confirm by model documentation; but our plan assumes CTC.

- Encapsulate the result in a struct (e.g., TextPrediction{text string, confidence float64}).

We will write **unit tests for the decoding logic**: e.g., create a dummy output where we know the argmax sequence and ensure our collapse logic works (like input \["a", "a", blank, "b"\] decodes to "ab"). Also test the confidence calc with a simple distribution.

**4.4. Batch Recognition Optimization (Optional):** If there are many text regions, we could batch some of them to run in one go through the model to speed up inference. ONNX Runtime and the model should handle batch >1 (the first dimension). We can accumulate, say, up to N crops of similar size, pad them to the same width, stack into a batch and run one forward pass. However, for simplicity initially, we will do one region at a time (batch size 1) which is easier and plenty fast for moderate number of detections. We can note this as a potential optimization. The structure of our code (Detector producing regions, then iterating recognizer on each) is fine to start.

### 5. Integrating the Pipeline

Now that detection and recognition pieces are in place, we integrate them in the main pipeline:

**5.1. OCR Pipeline Orchestration:** Create an OCR struct that contains a Detector and Recognizer (and possibly orientation classifier). Provide a builder or initializer function similar to OAROCRBuilder[\[27\]](https://docs.rs/oar-ocr/latest/oar_ocr/#:~:text=%2F%2F%20Build%20OCR%20pipeline%20let,to_string%28%29%2C%20%29.build) - it takes paths to detection model, recognition model, dict, and optional models, and instantiates the respective components. For example:

ocr := NewOCRPipeline(detModelPath, recModelPath, dictPath, options...)

This will load both models and be ready.

**5.2. end-to-end ProcessImage:** Implement a method like OCR.ProcessImage(img image.Image) -> OCRResult. This will: - Run Detector.Detect(img) to get text region proposals. - If **document orientation correction** is enabled (we'll detail in step 6), possibly adjust the image orientation before detection, or if done after detection, we might re-orient coordinates - but typically you'd rotate the whole image up front if needed. We will handle this in step 6. - For each detected region, run Recognizer.Recognize(regionCrop). - If **text line orientation** (angle of individual text lines) is enabled (optional), apply that (see step 6). - Collect all the recognized text results and their positions. Compose an OCRResult object containing a list of TextRegion objects, where each region has its bounding polygon (or box), the transcribed text, and confidence. Ensure each region is tagged with maybe an ID or index. OAR's result includes result.index for image index in batch[\[28\]](https://github.com/greatv/oar-ocr#:~:text=for%20result%20in%20results%20,text%2C%20confidence%29%3B); in our case, if processing one image at a time, index is trivial. For batch processing multiple images, we can handle that too: - We can provide ProcessImages(\[\]image.Image) -> \[\]OCRResult to process a batch by looping through images (since onnxruntime-go can run one at a time easily, no need for true parallel unless we decide to multi-thread). - OAR-OCR supports batch input natively[\[29\]](https://docs.rs/oar-ocr/latest/oar_ocr/#:~:text=%2F%2F%20Process%20multiple%20images%20let,result.index%2C%20result.text_regions.len%28%29%29%3B); our port can simply loop to achieve similar effect, or use multiple goroutines if we want concurrency (but then must be careful with onnxruntime thread-safety - safer to process sequentially or have separate session per thread).

**5.3. Output Format:** For the CLI, we will likely output results as JSON or text. Define a struct for JSON output (maybe reuse OCRResult which contains an array of regions, and each region has text, confidence, and bounding box coordinates). We will ensure this can be marshaled to JSON nicely (e.g., coordinates as an array \[x1,y1,x2,y2,...\] for polygons).

**5.4. Test the Integrated Pipeline:** Now, we perform a full pipeline test on a known image: - Use a sample image (from testdata) that contains some text. Run ocr.ProcessImage on it. - Check that the result makes sense: e.g., the output text strings should match expectation. If we have a known ground truth or we ran PaddleOCR on it separately, compare them. This could be a unit test assertion if we trust the reference output exactly. If exact match is tricky (OCR might have minor differences), we can at least assert that some expected keywords are found, or the number of detected lines is correct. - We will also test on an image with no text (blank image) to ensure it gracefully returns zero regions (no false positives ideally). - Memory test: process multiple images in a loop to see there are no memory leaks or excessive time (maybe incorporate Go benchmarks later). ONNX Runtime manages memory internally, but we should call session.Close() on shutdown.

### 6. Optional Enhancements: Orientation Correction and Rectification

_(This step is optional, but for completeness of "full pipeline", we outline it. These can be implemented after the core pipeline is working.)_

**6.1. Document Orientation Classifier:** If a scanned page might be rotated 90/180 degrees, we use the orientation model to correct it. The model (PPLCNet-based) takes the whole image and classifies it into one of 4 orientations[\[1\]](https://docs.rs/oar-ocr/latest/oar_ocr/#:~:text=,Document%20Rectification%3A%20Fix%20perspective%20distortion). We would do: - After loading the orientation ONNX model into a session, run it on the input image (perhaps resized to a standard size like 224x224 for the classifier). - Get the predicted angle. If the highest probability is above a threshold (say 0.8) and the angle is not 0°, then rotate the image accordingly (90, 180, or 270 degrees). Use imaging.Rotate90/180/270 to do this rotation. - Then feed the rotated image to the Detector as usual. If confidence is low or the classifier isn't used, default to assuming 0° (no rotation). - This logic can be integrated in OCR.ProcessImage if the feature is enabled (for example, OAR's builder has use_doc_orientation_classify(true) and a confidence threshold).

We should test this by manually rotating a test image and seeing if the classifier triggers and corrects it. For instance, take a normal text image, rotate it 90°, and check that our pipeline outputs the text in correct orientation after applying classifier.

**6.2. Text Line Orientation (Rotation) for Crops:** For languages like Chinese or when vertical text is present, PaddleOCR includes a text angle classifier per text line (to detect if a detected text region is vertical). If we include this: - Load the text-line orientation model (a lightweight classifier that likely outputs 0 or 90 degrees). - For each detected region crop (before recognition), run this classifier. If it predicts "rotated text" with high confidence, rotate the crop 90 degrees (and adjust expected output characters mapping if needed - but in practice, just rotate image so text becomes horizontal). - Then feed to recognizer. This ensures we can handle vertical text lines. - OAR's builder uses .use_textline_orientation(true) with a threshold[\[30\]](https://docs.rs/oar-ocr/latest/oar_ocr/#:~:text=.textline_orientation_classify_model_path%28,build) - we'll mimic that.

Test with a vertical text sample if available (or simulate by rotating a word and seeing if classifier catches it).

**6.3. Document Rectification:** This is an advanced step for cases where the page is warped (e.g., photo of a curled page). OAR lists a UVDoc model for this[\[31\]](https://github.com/greatv/oar-ocr#:~:text=Detect%20text%20line%20orientation%20Text,6MB%20Fix%20perspective%20distortion). If required: - Load the rectification model which presumably outputs a transformed image or mesh to correct the perspective. - Run it on the original image (before detection). If it outputs a rectified image, use that for OCR. If it outputs a deformation field, we'd have to apply that to the image. - This is complex; we may decide to skip implementing this in Go initially due to complexity. It can be a future enhancement if needed.

### 7. PDF Support

Once the image-based pipeline works, we add PDF input capability:

**7.1. Implement PDF to Image conversion:** Using pdfcpu, we will write a function ImagesFromPDF(pdfPath string) -> \[\]image.Image. This will: - Open the PDF (pdfcpu provides functions to parse a PDF file into a context). - Iterate through pages and for each, extract images. - If the PDF is a scanned doc, typically each page has a full-page image. Pdfcpu's ExtractImages can find all image objects. We might get multiple images if the PDF has multiple images per page (for example, some PDFs might have one image per scanned section). We will extract all and consider merging or choosing the largest if needed. - If the PDF has vector text (no images), then ExtractImages might return none. In that case, as an OCR tool, we could either skip or render the page. A simple approach: if no image found, we can **rasterize** the page to image using pdfcpu's page content. However, pdfcpu doesn't render to image. We might detect this scenario and either output nothing or require an external rasterizer. Since this is edge-case and OCR on vector text is unnecessary (better to extract text via PDF), we can document that PDF support expects scanned PDFs. - For each image bytes extracted, decode it to image.Image (pdfcpu likely gives us raw \[\]byte which could be JPEG or PNG data streams). We can use image.Decode on those bytes. - We might need to be careful about image resolution. If the embedded image is lower resolution, there's not much we can do. If the PDF has an image mask or separate color planes, pdfcpu might return them separately. We should test on a sample PDF to ensure we get the full page image properly.

**7.2. OCR on PDF pages:** For CLI, when a PDF file is input, use the above function to get images for each page. Then run our OCR pipeline on each page image. We can either output combined results or per page. For simplicity, we output per page (maybe as an array of page results). E.g., the JSON output could be { "pages": \[ { "number":1, "regions": \[ ... \]}, { "number":2, ...} \] }. Or we simply print text with page separators if user wants plain text.

**7.3. Test PDF Workflow:** Use a test PDF (we can create one by scanning a page or use one from an open dataset). Run the tool on it and verify that it extracts the same text as if we ran OCR on an equivalent image. For example, if we have a PDF with a known text snippet, verify that appears in output. We should also test a multi-page PDF to ensure loop works. And a PDF with no images to see how we handle it (likely just produce no output or a warning).

### 8. CLI Tool Implementation

Design a user-friendly CLI similar to PaddleOCR's. We can provide options like: - -image &lt;image_path&gt; or -pdf &lt;pdf_path&gt; (mutually exclusive, or we auto-detect by extension). - -output &lt;output_path&gt; if user wants to save output (or we just print to stdout). - -format &lt;plain|json&gt; to choose plain text (just lines of text) or JSON structured output (with coordinates). - Possibly options to toggle detection of orientation (-orient) or using faster/slower models etc.

Using **cobra** can simplify arg parsing and help support subcommands (if we ever add more features). But for now a single-command tool with flags is fine.

**Implementation details:** - Parse flags, load models (we can allow specifying custom model paths via flags, or use default bundled ones). - Create the OCR pipeline instance. - If input is an image file: load and run OCR, get result. - If input is a directory of images: loop through them (like PaddleOCR CLI can do batch). - If input is a PDF: use PDF logic, then process each page. - Format the output: - Plain text: we can just concatenate all recognized text lines (maybe separated by newlines or blank line between regions). However, plain text loses position info and reading order might be tricky if multiple columns, etc. For a simple approach, we can sort the detected regions top-to-bottom, left-to-right (approximate by bounding box coordinates) and output text in that order, each on a new line. - JSON: produce a structured output listing each region's text and coordinates. This is more useful for downstream processing (the user mentioned "paddle-json", likely referring to PaddleOCR's JSON output option). - Print or save the output accordingly.

**Testing the CLI:** - We can add a couple of integration tests that run the CLI (via os/exec.Command) on a sample image/PDF and capture the output, then verify it contains expected text. This is end-to-end. Alternatively, since our library functions are testable, CLI tests might be minimal. - Also test usage messages for invalid input (no file provided, etc.).

### 9. Comprehensive Testing Strategy

Throughout development, we will employ both **unit tests** and **integration tests**:

- **Unit Tests:** For all internal functions: image preprocessing, detection post-process, recognition decoding, etc., as described in prior sections. For instance:
- Test image normalization with a small known image (e.g., a 1x1 white pixel should produce a 1.0 in tensor).
- Test connected component and contour extraction on a synthetic mask.
- Test CTC decoding on fabricated model output arrays.
- Test orientation classifier by simulating an obvious rotated image (maybe a blank image with a simple pattern). These tests ensure each algorithmic piece works in isolation and edge cases are handled (e.g., no detection found, blank output from recognizer should be handled gracefully).
- **Integration Tests:** After assembling the pipeline:
- Test the pipeline on a known image. We can prepare an image with text "HELLO" and verify that the recognizer indeed outputs "HELLO". If exact match is expected, assert it. We might use an image generated from a font to be certain of the content.
- Use a real-world sample (like a scanned receipt or page) where we know some of the text, and check that the critical pieces are present in the output.
- If we have the original OAR-OCR or PaddleOCR available, generate reference outputs for a few images. For example, run PaddleOCR's Python API on a test image to get the texts/coordinates, and then assert our Go output matches closely (text match and maybe IoU of bounding boxes > some threshold). This can be done manually to verify correctness during development, and possibly encoded as an automated test if we store the expected output in a JSON file in testdata.
- PDF integration test: use a small PDF in testdata and ensure our code returns the same result as running the pipeline on each page's extracted image. This ensures the PDF image extraction didn't distort anything.
- **Performance Tests:** Not strictly required, but we will informally test speed (e.g., process a batch of images and measure time) to ensure the Go version with ONNX Runtime is within acceptable range of the Rust version. ONNX Runtime should give comparable performance across languages. If any hotspot in pure Go code is identified (like a slow contour-finding for very large images), we can optimize it (e.g., by using more efficient data structures or parallelizing it). We might add a Go benchmark for the detector on a size X image to track improvements.
- **Comparison Validation:** During development, we may use OAR-OCR (Rust) or PaddleOCR as a "ground truth" oracle:
- For example, take 5 diverse test images, run them through OAR-OCR in a Rust test program to get outputs (text and maybe region count).
- Then run our Go code on the same and compare. If differences arise, inspect why (e.g., decoding difference or missed detection) and adjust thresholds or fix bugs until they align. This gives high confidence in parity.
- If feasible, we could even call the Rust library from Go via cgo for automated comparison (by compiling Rust into a C-callable library). This is complex and likely unnecessary if manual comparisons suffice, but it was mentioned as an idea. Simpler is to use known outputs as mentioned.
- **Edge cases:** Test with images with no text, very small text, very large images (to see if memory scales, possibly test that our resizing logic downsizes them properly), non-Latin text if relevant (requires using a model and dict for that language, but we can at least ensure code isn't ASCII-specific - e.g., ensure UTF-8 handling in output strings).

All tests will be run in CI to catch regressions. We aim for a high coverage, especially for the critical text decoding and detection logic, as those are prone to subtle bugs.

## Timeline & Milestones (Summary of Plan Phases)

- **Initial Setup & Demo (Week 1):** Get onnxruntime-go working and run a simple inference on a known model. Load a detection model and run on a static image to ensure the pipeline skeleton works (even if we just get raw output).
- **Detection Module (Week 2):** Implement preprocessing and detection post-processing. Test with sample images until detection boxes seem reasonable.
- **Recognition Module (Week 3):** Implement recognition inference and CTC decoding. Test on isolated cropped text images (you can use detector's output or manual crops) to validate text output.
- **Integrate Pipeline (Week 4):** Tie detection and recognition. Achieve end-to-end OCR on single images. Compare results with expected outputs; refine thresholds or fixes. At this stage, we should have a functioning OCR for images.
- **Orientation & Refinements (Week 5):** Add orientation classifier and text-line rotation if needed. Improve detection post-processing (e.g., use better polygon extraction or "unclip" to get tighter boxes).
- **PDF Support & CLI (Week 6):** Implement PDF image extraction and finalize CLI interface. Ensure the tool works as a drop-in OCR solution for both image files and PDFs.
- **Testing & Polishing (Week 7):** Write any remaining tests, improve documentation (usage instructions for the CLI, code comments), optimize any slow parts, and verify memory usage (use Go profiler if needed).
- **Comparison & Deployment (Week 8):** Do final comparisons with PaddleOCR/OAR on a variety of documents to be confident in accuracy. Then prepare for release (binary build or container for server usage, etc.).

_(The timeline is illustrative; tasks can be parallelized or adjusted as needed.)_

## Status Update

✅ **Phase 1.3 Complete**: All required ONNX models (PP-OCRv5 detection, recognition, orientation classifiers, and UVDoc rectification) are downloaded and verified compatible with ONNX Runtime v1.23.0.

**Available Models:**

- Detection: PP-OCRv5 mobile (4.5MB) & server (84MB) - Input: [batch, 3, H, W], Output: [batch, 1, H, W]
- Recognition: PP-OCRv5 mobile (15.8MB) & server (80.6MB) - Input: [batch, 3, 48, W], 18,385 classes
- Orientation: Document (6.4MB) & text line classifiers (0.9-6.4MB) for rotation correction
- Rectification: UVDoc (30.1MB) for perspective correction

## Conclusion

By following this plan, we will create a Go-based OCR pipeline equivalent to OAR-OCR. The solution will use ONNX Runtime via Go bindings for robust and efficient model inference[\[7\]](https://github.com/yalue/onnxruntime_go#:~:text=This%20library%20seeks%20to%20provide,simple%20to%20use%20as%20possible), and mirror the proven PaddleOCR models for high accuracy[\[2\]](https://github.com/greatv/oar-ocr#:~:text=,engine%20in%20this%20OCR%20library). We'll carefully implement image processing and OCR algorithms in Go, leveraging libraries like pdfcpu for PDF handling[\[14\]](https://pkg.go.dev/github.com/hhrutter/pdfcpu/extract#:~:text=Overview%20%C2%B6). Comprehensive tests will ensure the port is correct and reliable. The end result will be an OCR library and CLI tool in pure Go (with optional cgo dependencies) that can detect and recognize text from images/PDFs, comparable to PaddleOCR's functionality but integrated into a Go ecosystem. This sets the stage for easy deployment in Go servers and use as a command-line tool (ocrgo for example) for document processing tasks.

By gradually substituting any remaining cgo parts with native Go (once we verify everything), we adhere to the preference for a pure-Go solution. GoMLX or similar frameworks might in the future allow us to load the model graph natively and eliminate ONNX Runtime - this can be explored once the baseline is achieved. For now, the focus is on delivering a working, tested pipeline. With this plan, the development will proceed in a structured way, reducing risk and ensuring we maintain parity with the original OCR capabilities at each step.

**Sources:**

- OAR-OCR uses PaddleOCR's ONNX models for detection/recognition[\[2\]](https://github.com/greatv/oar-ocr#:~:text=,engine%20in%20this%20OCR%20library), and provides text detection & recognition as separate components[\[1\]](https://docs.rs/oar-ocr/latest/oar_ocr/#:~:text=,Document%20Rectification%3A%20Fix%20perspective%20distortion).
- onnxruntime-go library will be used to load and execute ONNX models in Go[\[7\]](https://github.com/yalue/onnxruntime_go#:~:text=This%20library%20seeks%20to%20provide,simple%20to%20use%20as%20possible). This wraps the ONNX Runtime C API for inference.
- GoMLX offers a potential pure-Go path as "a PyTorch/Jax/TensorFlow for Go" with XLA backend[\[9\]](https://github.com/gomlx/gomlx#:~:text=GoMLX%20is%20an%20easy,as%20a%20PyTorch%2FJax%2FTensorFlow%20for%20Go) (future consideration for removing cgo).
- pdfcpu library can extract images from PDF pages in pure Go[\[14\]](https://pkg.go.dev/github.com/hhrutter/pdfcpu/extract#:~:text=Overview%20%C2%B6), enabling PDF input support for the OCR tool.

[\[1\]](https://docs.rs/oar-ocr/latest/oar_ocr/#:~:text=,Document%20Rectification%3A%20Fix%20perspective%20distortion) [\[6\]](https://docs.rs/oar-ocr/latest/oar_ocr/#:~:text=,Runtime%20integration%20for%20fast%20inference) [\[17\]](https://docs.rs/oar-ocr/latest/oar_ocr/#:~:text=%C2%A7Components) [\[21\]](https://docs.rs/oar-ocr/latest/oar_ocr/#:~:text=%2F%2F%20Process%20single%20image%20let,0) [\[22\]](https://docs.rs/oar-ocr/latest/oar_ocr/#:~:text=%2F%2F%20Process%20multiple%20images%20let,result.index%2C%20result.text_regions.len) [\[24\]](https://docs.rs/oar-ocr/latest/oar_ocr/#:~:text=,build) [\[27\]](https://docs.rs/oar-ocr/latest/oar_ocr/#:~:text=%2F%2F%20Build%20OCR%20pipeline%20let,to_string%28%29%2C%20%29.build) [\[29\]](https://docs.rs/oar-ocr/latest/oar_ocr/#:~:text=%2F%2F%20Process%20multiple%20images%20let,result.index%2C%20result.text_regions.len%28%29%29%3B) [\[30\]](https://docs.rs/oar-ocr/latest/oar_ocr/#:~:text=.textline_orientation_classify_model_path%28,build) oar_ocr - Rust

<https://docs.rs/oar-ocr/latest/oar_ocr/>

[\[2\]](https://github.com/greatv/oar-ocr#:~:text=,engine%20in%20this%20OCR%20library) [\[3\]](https://github.com/greatv/oar-ocr#:~:text=Model%20Type%20Version%20Category%20Model,precision%20requirements) [\[4\]](https://github.com/greatv/oar-ocr#:~:text=) [\[5\]](https://github.com/greatv/oar-ocr#:~:text=Character%20Dictionaries) [\[16\]](https://github.com/greatv/oar-ocr#:~:text=match%20at%20L476%20Character%20Dictionary,OCRv5%20Eastern%20Slavic%20models) [\[18\]](https://github.com/greatv/oar-ocr#:~:text=%23%20Language) [\[19\]](https://github.com/greatv/oar-ocr#:~:text=Model%20Type%20Version%20Category%20Model,6MB%20Fix%20perspective%20distortion) [\[25\]](https://github.com/greatv/oar-ocr#:~:text=for%20text_region%20in%20%26result.text_regions%20,text%2C%20confidence%29%3B%20%7D) [\[26\]](https://github.com/greatv/oar-ocr#:~:text=for%20text_region%20in%20%26result.text_regions%20,) [\[28\]](https://github.com/greatv/oar-ocr#:~:text=for%20result%20in%20results%20,text%2C%20confidence%29%3B) [\[31\]](https://github.com/greatv/oar-ocr#:~:text=Detect%20text%20line%20orientation%20Text,6MB%20Fix%20perspective%20distortion) GitHub - GreatV/oar-ocr: A comprehensive OCR library, built in Rust with ONNX Runtime for efficient inference.

<https://github.com/greatv/oar-ocr>

[\[7\]](https://github.com/yalue/onnxruntime_go#:~:text=This%20library%20seeks%20to%20provide,simple%20to%20use%20as%20possible) [\[8\]](https://github.com/yalue/onnxruntime_go#:~:text=multiple%20Tensor%20data%20types%3B%20see,functions) [\[20\]](https://github.com/yalue/onnxruntime_go#:~:text=Note%20on%20onnxruntime%20Library%20Versions) [\[23\]](https://github.com/yalue/onnxruntime_go#:~:text=Additionally%2C%20this%20library%20uses%20Go%27s,functions) GitHub - yalue/onnxruntime_go: A Go (golang) library wrapping microsoft/onnxruntime.

<https://github.com/yalue/onnxruntime_go>

[\[9\]](https://github.com/gomlx/gomlx#:~:text=GoMLX%20is%20an%20easy,as%20a%20PyTorch%2FJax%2FTensorFlow%20for%20Go) [\[10\]](https://github.com/gomlx/gomlx#:~:text=It%20runs%20almost%20everywhere%20Go,see%20Tamago) GitHub - gomlx/gomlx: GoMLX: An Accelerated Machine Learning Framework For Go

<https://github.com/gomlx/gomlx>

[\[11\]](https://github.com/oramasearch/onnx-go#:~:text=The%20implementation%20of%20the%20the,existent%20for%20the%20export) [\[12\]](https://github.com/oramasearch/onnx-go#:~:text=import%20%22github.com%2Fowulveryck%2Fonnx) GitHub - oramasearch/onnx-go: onnx-go gives the ability to import a pre-trained neural network within Go without being linked to a framework or library.

<https://github.com/oramasearch/onnx-go>

[\[13\]](https://github.com/pdfcpu/pdfcpu#:~:text=pdfcpu%2Fpdfcpu%3A%20A%20PDF%20processor%20written,line%20interface%20%28CLI) pdfcpu/pdfcpu: A PDF processor written in Go. - GitHub

<https://github.com/pdfcpu/pdfcpu>

[\[14\]](https://pkg.go.dev/github.com/hhrutter/pdfcpu/extract#:~:text=Overview%20%C2%B6) extract package - github.com/hhrutter/pdfcpu/extract - Go Packages

<https://pkg.go.dev/github.com/hhrutter/pdfcpu/extract>

[\[15\]](https://www.libhunt.com/r/go-fitz#:~:text=Go,The%20primary) Go-fitz Alternatives and Reviews - LibHunt

<https://www.libhunt.com/r/go-fitz>
